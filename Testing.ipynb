{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solar Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb scikit-learn tensorflow_probability tensorflow_addons matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Add, Dense, Dropout, Layer, LayerNormalization, MultiHeadAttention, Normalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.utils import timeseries_dataset_from_array\n",
    "from tensorflow.keras.metrics import MeanSquaredError, RootMeanSquaredError\n",
    "from tensorflow_addons.metrics import RSquare\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_4d(matrix):\n",
    "    fig = plt.figure(figsize=(10, 20), dpi=300)\n",
    "    plt.title(\"Attention heatmap\")\n",
    "\n",
    "    # create grid\n",
    "    x = np.arange(0, matrix.shape[0], 1, dtype=np.int32)  # timesteps\n",
    "    y = np.arange(0, matrix.shape[1], 1, dtype=np.int32)    # patches\n",
    "    z = np.arange(0, matrix.shape[2], 1, dtype=np.int32)  # timesteps\n",
    "    X, Y, Z = np.meshgrid(x, y, z)\n",
    "\n",
    "    X = X.transpose([1, 0, 2])\n",
    "    Y = Y.transpose([1, 0, 2])\n",
    "    Z = Z.transpose([1, 0, 2])\n",
    "\n",
    "    for I in range(matrix.shape[3]):\n",
    "        # Plot\n",
    "        ax = plt.subplot(5, 5, I+1, projection=\"3d\")\n",
    "        ax.scatter3D(X, Y, Z, c=matrix[:, :, :, I], marker='s', s=99, cmap='magma')\n",
    "        ax.set_title(\n",
    "            f\"{I}-th patch\"\n",
    "        )\n",
    "        ax.set_xlabel(\"Timestep\")\n",
    "        ax.set_ylabel(\"Patch\")\n",
    "        ax.set_zlabel(\"Timestep\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Timestep, Patch)\n",
    "def create_look_ahead_mask(size):\n",
    "  n = tf.cast((size * (size+1) / 2), dtype=tf.int32)\n",
    "  mask = tfp.math.fill_triangular(tf.ones((n,), dtype=tf.bool), upper=False)\n",
    "  mask = mask[:, tf.newaxis, :, tf.newaxis]  # (timestep, 1, timestep, 1)\n",
    "  return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_missing_values_mask(seq):\n",
    "  seq = tf.math.reduce_all(tf.math.greater_equal(seq, 0), axis=-1)\n",
    "  return seq[:, tf.newaxis, tf.newaxis, tf.newaxis, :, :]  # (batch_size, 1, 1, 1, timesteps, patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(Layer):\n",
    "    def __init__(self, units, dropout_rate, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "\n",
    "        self.units = units\n",
    "\n",
    "        self.projection = Dense(units, kernel_initializer=TruncatedNormal(stddev=0.02))\n",
    "        self.dropout = Dropout(rate=dropout_rate)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(PositionalEmbedding, self).build(input_shape)\n",
    "\n",
    "        print(input_shape)\n",
    "        self.position = self.add_weight(\n",
    "            name=\"position\",\n",
    "            shape=(1, input_shape[1], input_shape[2], self.units),\n",
    "            initializer=TruncatedNormal(stddev=0.02),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "\n",
    "        x = self.projection(inputs)\n",
    "        x = x + self.position[:, :seq_len, :, :]\n",
    "\n",
    "        return self.dropout(x, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Layer):\n",
    "    def __init__(\n",
    "        self, embed_dim, mlp_dim, num_heads, dropout_rate, attention_dropout_rate, **kwargs\n",
    "    ):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "\n",
    "        # MultiHeadAttention\n",
    "        self.mha = MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embed_dim,\n",
    "            dropout=attention_dropout_rate,\n",
    "            kernel_initializer=TruncatedNormal(stddev=0.02),\n",
    "            attention_axes=(1, 2),       # 2D attention (timestep, patch)\n",
    "        )\n",
    "\n",
    "        # Point wise feed forward network\n",
    "        self.dense_0 = Dense(\n",
    "            units=mlp_dim,\n",
    "            activation=\"gelu\",\n",
    "            kernel_initializer=TruncatedNormal(stddev=0.02),\n",
    "        )\n",
    "        self.dense_1 = Dense(\n",
    "            units=embed_dim, kernel_initializer=TruncatedNormal(stddev=0.02)\n",
    "        )\n",
    "\n",
    "        self.dropout_0 = Dropout(rate=dropout_rate)\n",
    "        self.dropout_1 = Dropout(rate=dropout_rate)\n",
    "\n",
    "        self.norm_0 = LayerNormalization(epsilon=1e-5)\n",
    "        self.norm_1 = LayerNormalization(epsilon=1e-5)\n",
    "\n",
    "        self.add_0 = Add()\n",
    "        self.add_1 = Add()\n",
    "\n",
    "    def call(self, inputs, training, attention_mask):\n",
    "        # Attention block\n",
    "        x = self.norm_0(inputs)\n",
    "        x = self.mha(\n",
    "            query=x,\n",
    "            key=x,\n",
    "            value=x,\n",
    "            training=training,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        x = self.dropout_0(x, training=training)\n",
    "        x = self.add_0([x, inputs])\n",
    "\n",
    "        # MLP block\n",
    "        y = self.norm_1(x)\n",
    "        y = self.dense_0(y)\n",
    "        y = self.dense_1(y)\n",
    "        y = self.dropout_1(y, training=training)\n",
    "\n",
    "        return self.add_1([x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Layer):\n",
    "    def __init__(\n",
    "        self, embed_dim, mlp_dim, num_heads, dropout_rate, attention_dropout_rate, **kwargs\n",
    "    ):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "\n",
    "        # MultiHeadAttention\n",
    "        self.mha_0 = MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embed_dim,\n",
    "            dropout=attention_dropout_rate,\n",
    "            kernel_initializer=TruncatedNormal(stddev=0.02),\n",
    "            attention_axes=(1, 2),          # 2D attention (timestep, patch)\n",
    "        )\n",
    "        self.mha_1 = MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embed_dim,\n",
    "            dropout=attention_dropout_rate,\n",
    "            kernel_initializer=TruncatedNormal(stddev=0.02),\n",
    "            attention_axes=(1, 2),          # 2D attention (timestep, patch)\n",
    "        )\n",
    "\n",
    "        # Point wise feed forward network\n",
    "        self.dense_0 = Dense(\n",
    "            units=mlp_dim,\n",
    "            activation=\"gelu\",\n",
    "            kernel_initializer=TruncatedNormal(stddev=0.02),\n",
    "        )\n",
    "        self.dense_1 = Dense(\n",
    "            units=embed_dim, kernel_initializer=TruncatedNormal(stddev=0.02)\n",
    "        )\n",
    "\n",
    "        self.dropout_0 = Dropout(rate=dropout_rate)\n",
    "        self.dropout_1 = Dropout(rate=dropout_rate)\n",
    "        self.dropout_2 = Dropout(rate=dropout_rate)\n",
    "\n",
    "        self.norm_0 = LayerNormalization(epsilon=1e-5)\n",
    "        self.norm_1 = LayerNormalization(epsilon=1e-5)\n",
    "        self.norm_2 = LayerNormalization(epsilon=1e-5)\n",
    "\n",
    "        self.add_0 = Add()\n",
    "        self.add_1 = Add()\n",
    "        self.add_2 = Add()\n",
    "\n",
    "    def call(self, inputs, enc_output, training, look_ahead_mask, attention_mask):\n",
    "        # Attention block\n",
    "        x = self.norm_0(inputs)\n",
    "        x, block1 = self.mha_0(\n",
    "            query=x,\n",
    "            key=x,\n",
    "            value=x,\n",
    "            training=training,\n",
    "            attention_mask=look_ahead_mask,\n",
    "            return_attention_scores=True,\n",
    "        )\n",
    "        x = self.dropout_0(x, training=training)\n",
    "        x = self.add_0([x, inputs])\n",
    "\n",
    "        # Attention block\n",
    "        y = self.norm_1(x)\n",
    "        y, block2 = self.mha_1(\n",
    "            query=y,\n",
    "            key=enc_output,\n",
    "            value=enc_output,\n",
    "            training=training,\n",
    "            attention_mask=attention_mask,\n",
    "            return_attention_scores=True,\n",
    "        )\n",
    "        y = self.dropout_1(y, training=training)\n",
    "        y = self.add_1([x, y])\n",
    "\n",
    "        # MLP block\n",
    "        z = self.norm_2(y)\n",
    "        z = self.dense_0(z)\n",
    "        z = self.dense_1(z)\n",
    "        z = self.dropout_2(z, training=training)\n",
    "\n",
    "        return self.add_2([y, z]), block1, block2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        embed_dim,\n",
    "        mlp_dim,\n",
    "        num_heads,\n",
    "        num_outputs,\n",
    "        dropout_rate,\n",
    "        attention_dropout_rate,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(Transformer, self).__init__(**kwargs)\n",
    "\n",
    "        # Input (normalization of RAW measurements)\n",
    "        self.input_norm_enc = Normalization()\n",
    "        self.input_norm_dec = Normalization()\n",
    "\n",
    "        # Input\n",
    "        self.pos_embs_0 = PositionalEmbedding(embed_dim, dropout_rate)\n",
    "        self.pos_embs_1 = PositionalEmbedding(embed_dim, dropout_rate)\n",
    "\n",
    "        # Encoder\n",
    "        self.enc_layers = [\n",
    "            Encoder(embed_dim, mlp_dim, num_heads, dropout_rate, attention_dropout_rate)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ]\n",
    "        self.norm_0 = LayerNormalization(epsilon=1e-5)\n",
    "\n",
    "        # Decoder\n",
    "        self.dec_layers = [\n",
    "            Decoder(embed_dim, mlp_dim, num_heads, dropout_rate, attention_dropout_rate)\n",
    "            for _ in range(num_decoder_layers)\n",
    "        ]\n",
    "        self.norm_1 = LayerNormalization(epsilon=1e-5)\n",
    "\n",
    "        # Output\n",
    "        self.final_layer = Dense(num_outputs, kernel_initializer=TruncatedNormal(stddev=0.02))\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        inputs, targets   = inputs\n",
    "        attention_weights = {}\n",
    "\n",
    "        missing_values_mask, look_ahead_mask = self.create_masks(inputs, targets)\n",
    "\n",
    "        # Encoder input\n",
    "        x_e = self.input_norm_enc(inputs)\n",
    "        x_e = self.pos_embs_0(x_e, training=training)\n",
    "\n",
    "        # Decoder input\n",
    "        x_d = self.input_norm_dec(targets)\n",
    "        x_d = self.pos_embs_1(x_d, training=training)\n",
    "\n",
    "        # Encoder\n",
    "        for layer in self.enc_layers:\n",
    "            x_e = layer(x_e, training=training, attention_mask=missing_values_mask)\n",
    "        x_e = self.norm_0(x_e)\n",
    "\n",
    "        # Decoder\n",
    "        for i, layer in enumerate(self.dec_layers):\n",
    "            x_d, block1, block2 = layer(x_d, x_e, training=training, look_ahead_mask=look_ahead_mask, attention_mask=missing_values_mask)\n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "        x_d = self.norm_1(x_d)\n",
    "\n",
    "        #plot_4d(attention_weights[f'decoder_layer{1}_block1'][0, 0, :, :, :, :])\n",
    "        #plot_4d(attention_weights[f'decoder_layer{1}_block2'][0, 0, :, :, :, :])\n",
    "\n",
    "        # Output\n",
    "        final_output = self.final_layer(x_d)\n",
    "\n",
    "        return final_output, attention_weights\n",
    "    \n",
    "    def train_step(self, inputs):\n",
    "        inputs, targets = inputs\n",
    "        inputs = inputs[:, :-1]\n",
    "        targets_inputs = targets[:, :-1]\n",
    "        targets_real = targets[:, 1:]\n",
    "        mask = tf.cast(tf.math.greater_equal(targets_real, 0), dtype=tf.int32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred, _ = self([inputs, targets_inputs], training=True)\n",
    "            loss = self.compiled_loss(targets_real, y_pred, sample_weight=mask, regularization_losses=self.losses)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(targets_real, y_pred, sample_weight=mask)\n",
    "\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def test_step(self, inputs):\n",
    "        inputs, targets = inputs\n",
    "        inputs = inputs[:, :-1]\n",
    "        targets_inputs = targets[:, :-1]\n",
    "        targets_real = targets[:, 1:]\n",
    "        mask = tf.cast(tf.math.greater_equal(targets_real, 0), dtype=tf.int32)\n",
    "\n",
    "        # Compute predictions\n",
    "        y_pred, _ = self([inputs, targets_inputs], training=False)\n",
    "\n",
    "        # Updates the metrics tracking the loss\n",
    "        self.compiled_loss(targets_real, y_pred, sample_weight=mask, regularization_losses=self.losses)\n",
    "\n",
    "        # Update the metrics.\n",
    "        self.compiled_metrics.update_state(targets_real, y_pred, sample_weight=mask)\n",
    "\n",
    "        # Return a dict mapping metric names to current value.\n",
    "        # Note that it will include the loss (tracked in self.metrics).\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def create_masks(self, inputs, targets):\n",
    "        # Encoder padding mask (Used in the 2nd attention block in the decoder too.)\n",
    "        missing_values_mask = create_missing_values_mask(inputs)\n",
    "\n",
    "        # Used in the 1st attention block in the decoder.\n",
    "        look_ahead_mask = create_look_ahead_mask(tf.shape(targets)[1])\n",
    "        dec_target_missing_values_mask = create_missing_values_mask(targets)\n",
    "        look_ahead_mask = tf.logical_and(dec_target_missing_values_mask, look_ahead_mask)\n",
    "\n",
    "        return missing_values_mask, look_ahead_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(window_size, batch_size):\n",
    "    # load dataset\n",
    "    f = np.load('./dataset/dataset-3.npz')\n",
    "    X_all = f['X']\n",
    "    y_daily_all = f['y_daily']\n",
    "    y_hourly_all = f['y_hourly']\n",
    "    print(X_all.shape, y_daily_all.shape, y_hourly_all.shape)\n",
    "\n",
    "    plt.plot(X_all[:720, 0, -2])\n",
    "    plt.plot(X_all[:720, 0, -1])\n",
    "    plt.plot(y_daily_all[:720, 0, 0])\n",
    "    plt.plot(y_daily_all[:720, 0, 1])\n",
    "    plt.plot(y_daily_all[:720, 0, 2])\n",
    "    plt.xlabel('Time [day]')\n",
    "    plt.title('Time of year signal - target')\n",
    "    plt.legend(['Region - Year sin', 'Region - Year cos', 'Target - Irradiance', 'Target - Year sin', 'Target - Year cos'])\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(y_hourly_all[6223:6500, 0, 0]/1000)\n",
    "    plt.plot(y_hourly_all[6223:6500, 0, 1])\n",
    "    plt.plot(y_hourly_all[6223:6500, 0, 2])\n",
    "    plt.plot(y_hourly_all[6223:6500, 0, 3])\n",
    "    plt.plot(y_hourly_all[6223:6500, 0, 4])\n",
    "    plt.xlabel('Time [h]')\n",
    "    plt.title('Time of day signal - target')\n",
    "    plt.legend(['Irradiance', 'Year sin', 'Year cos', 'Day sin', 'Day cos'])\n",
    "    plt.show()\n",
    "\n",
    "    # Split the data\n",
    "    X_train = X_all[0:int(X_all.shape[0]*0.8)]\n",
    "    X_val = X_all[int(X_all.shape[0]*0.8):int(X_all.shape[0]*0.9)]\n",
    "    print(X_train.shape, X_val.shape)\n",
    "\n",
    "    y_train = y_daily_all[0:int(y_daily_all.shape[0]*0.8)]\n",
    "    y_val = y_daily_all[int(y_daily_all.shape[0]*0.8):int(y_daily_all.shape[0]*0.9)]\n",
    "    print(y_train.shape, y_val.shape)\n",
    "\n",
    "    # Descriptive Statistics\n",
    "    print(f\"Minimum: {np.min(X_train)}\")\n",
    "    print(f\"Maximum: {np.max(X_train)}\")\n",
    "    print(f\"Mean: {np.mean(X_train)}\")\n",
    "    print(f\"Standard deviation: {np.std(X_train)}\\n\")\n",
    "\n",
    "    # Descriptive Statistics\n",
    "    print(f\"Minimum: {np.min(y_train)}\")\n",
    "    print(f\"Maximum: {np.max(y_train)}\")\n",
    "    print(f\"Mean: {np.mean(y_train)}\")\n",
    "    print(f\"Standard deviation: {np.std(y_train)}\\n\")\n",
    "\n",
    "    train_dataset = timeseries_dataset_from_array(\n",
    "        data=X_train,\n",
    "        targets=y_train,\n",
    "        sequence_length=window_size,\n",
    "        sequence_stride=1,        # 1 day\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        many_to_many=True\n",
    "    )\n",
    "    print(train_dataset)\n",
    "\n",
    "    val_dataset = timeseries_dataset_from_array(\n",
    "        data=X_val,\n",
    "        targets=y_val,\n",
    "        sequence_length=window_size,\n",
    "        sequence_stride=1,        # 1 day\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        many_to_many=True\n",
    "    )\n",
    "    print(val_dataset)\n",
    "\n",
    "    for i, (x, y) in enumerate(train_dataset):\n",
    "      plt.plot(x[0, :720, 0, -2])\n",
    "      plt.plot(x[0, :720, 0, -1])\n",
    "      plt.plot(y[0, :720, 0, 1])\n",
    "      plt.plot(y[0, :720, 0, 2])\n",
    "      plt.xlabel('Time [day]')\n",
    "      plt.title('Time of year signal - target')\n",
    "      if i > 5: break\n",
    "    plt.show()\n",
    "    \n",
    "    return train_dataset, val_dataset, (X_train, y_train)\n",
    "\n",
    "load_dataset(window_size=7, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulator(tf.Module):\n",
    "  def __init__(self, transformer):\n",
    "    self.transformer = transformer\n",
    "\n",
    "  def __call__(self, inputs, horizon_length):\n",
    "    inputs, targets = inputs\n",
    "    output_array = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "\n",
    "    for i in tf.range(horizon_length):\n",
    "      tar = targets[:, i:]\n",
    "      print(tar[0])\n",
    "      \n",
    "      # Concatenate history with the predicted future\n",
    "      if i > 0:\n",
    "        output = tf.transpose(output_array.stack(), perm=[1, 0, 2, 3])\n",
    "        tar = tf.concat([tar, output], axis=1)\n",
    "        print(tar[0])\n",
    "\n",
    "      predictions, _ = self.transformer([inputs, tar], training=False)\n",
    "\n",
    "      # select the last prediction\n",
    "      predictions = predictions[:, -1, :, :]  # (batch_size, patch, features)\n",
    "\n",
    "      # concatentate the prediction to the output which is given to the decoder\n",
    "      # as its input.\n",
    "      output_array = output_array.write(i, predictions)\n",
    "\n",
    "    output = tf.transpose(output_array.stack(), perm=[1, 0, 2, 3])\n",
    "    print(output.shape)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_encoder_layers=1, num_decoder_layers=1, embed_dim=128, num_heads=6, mlp_dim=256,\n",
    "    num_outputs=1, dropout_rate=0.1, attention_dropout_rate=0.1\n",
    ")\n",
    "simulator = Simulator(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 4\n",
    "window_size = 21\n",
    "batch_size = 64\n",
    "\n",
    "# load dataset\n",
    "_, _, test_dataset = load_dataset(window_size=(window_size + horizon - 1))\n",
    "\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs in test_dataset:\n",
    "    inputs, targets = inputs\n",
    "    inputs = inputs[:, :-horizon]\n",
    "    targets_inputs = targets[:, :-horizon]\n",
    "\n",
    "    y_pred, _ = transformer([inputs, targets_inputs], training=False)\n",
    "    break\n",
    "\n",
    "transformer.load_weights('./models/model-best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_obj = MeanSquaredError()\n",
    "rmse_obj = RootMeanSquaredError()\n",
    "r_square = RSquare(y_shape=(horizon, 25, 1))\n",
    "metrics = [mse_obj, rmse_obj, r_square]\n",
    "\n",
    "for inputs in test_dataset:\n",
    "    inputs, targets = inputs\n",
    "    inputs = inputs[:, :-horizon]\n",
    "    targets_inputs = targets[:, :-horizon]\n",
    "    targets_real = targets[:, -horizon:]\n",
    "    mask = tf.math.greater_equal(targets_real, 0)\n",
    "\n",
    "    y_pred = simulator([inputs, targets_inputs], horizon)\n",
    "\n",
    "    # get error of the last prediction\n",
    "    for m in metrics:\n",
    "        m.update_state(targets_real, y_pred, sample_weight=mask)\n",
    "\n",
    "    break\n",
    "# Return a dict mapping metric names to current value.\n",
    "# Note that it will include the loss (tracked in self.metrics).\n",
    "result = {m.name: m.result().numpy() for m in metrics}\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9185113d2128201d66faecd4f34fb34e89a635073a034991399523e584519355"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
